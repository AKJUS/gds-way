---
title: How to store and query logs
expires: 2018-03-01
---

# <%= current_page.data.title %>

<%= partial :expires %>

You should use the shared GDS [Logit](https://logit.io/) account to store and query infrastructure and application logs.

Logit offers cloud-based on-demand, shared [ELK (Elasticsearch, Logstash and Kibana)](https://www.elastic.co/elk-stack) stacks as a service. They are suitable for short-term storage of logs, which are accessible and queryable. The [Reliability Engineering documentation](https://reliability-engineering.cloudapps.digital/#logging) has more information about using Logit.

You should not operate your own self-hosted ELK stacks for logging. In the past we’ve found ELK relatively easy to set up, but in the long term you should use a cloud-hosted ELK solution.

## Store your logs

Your product may have legal or other requirements determining how long you should store logs. For example, the [Payment Card Industry Data Security Standard (PCI-DSS)](https://www.pcisecuritystandards.org/pci_security/) requires 3 months of easily accessible logs and 12 months of archived logs.

### Short-term storage

You should not need to store logs spanning long periods in your short-term queryable store. Practical retention periods for short-term queryable logs are:

* non-production environments - no more than 7 days
* production environments - no more than 30 days

### Long-term storage

When storing logs, you should focus on long-term durability. It’s good practice to take logs from a long-term store and load them into a queryable tool. For example, a one-time-use ELK stack.

You should be aware that ELK is not appropriate for long- term storage of logs. There are other services which are less expensive and more reliable. If you use Amazon Web Services (AWS), you could use [Amazon CloudWatch](https://aws.amazon.com/cloudwatch/) to run basic queries on older logs which may be a suitable solution.

## Log shipping

If you have set up log shipping, you should consider:

* how your logs will get to short and long-term stores?
* what happens if one of these stores is unavailable?
* whether the store will be overloaded when it comes back online?

You can use the [Elastic Beats](https://www.elastic.co/products/beats) family of tools which can detect periods of unavailability. They recall how much data they send and continue where they left off. By using a protocol which is backpressure-aware they also won’t overload [Logstash](https://www.elastic.co/products/logstash) when it comes back online.

For example, you could have one process to ship logs to your long-term archive, and a second process to fill a short-term query tool from the long-term archive. This way you can be confident everything is safely stored.
